{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intro\n",
        "- `inference_loader_with_flag()`: \n",
        "    - For processed video (Sonosite) folders, input 3 frames t1/t2/t3, view them as L/C/R (left/center/right), and predict the needle (`x1 y1` & `x3 y3`) at C.\n",
        "    - Call PK function to output **`raw_z1`** & **`raw_z3`** predicted by regression.\n",
        "- `線段檢查()`: Set the `find_train_end_left` & `find_train_end_right`\n",
        "- `PK()`: Regression on L & R bright & shadow area average value from `find_train_end_left` to `find_train_end_right`\n",
        "- `save_pred_to_plot()`: save inference & PK result plot to folder (if prediction mask is empty, save nothing)\n",
        "- `save_pred_to_json()`: save x,y prediction to json\n",
        "\n",
        "\n",
        "- TODO (might not be needed)\n",
        "    - process raw video (developer/prodigy管理/zipper array data for PK)crop 38mm\n",
        "    - show_3D()\n",
        "    - \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Packages & Read Config File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @Packages\n",
        "import json, os, random, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import concurrent.futures   ## Threading\n",
        "import time\n",
        "\n",
        "## model\n",
        "from model import Mask2Former\n",
        "# try:\n",
        "from model.memmask2former.inference_wrapper import MemInferenceWrapper\n",
        "from model.memmask2former.mem_m2f import track_model_cfg\n",
        "# except:\n",
        "#     pass\n",
        "\n",
        "from lib.config_helper import merge\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset import UnlabeledDataset, Augmentation\n",
        "\n",
        "import cv2\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import explained_variance_score\n",
        "from joblib import parallel_backend\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as tf\n",
        "from torchvision.transforms import v2\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "import omegaconf\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(suppress=True)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Read the user settings\n",
        "config_PK = OmegaConf.load(\"./configs/config_PK.yml\")\n",
        "\n",
        "run_only_one = False      ## Inference a single frame\n",
        "preprocess_video = False  ## Assume frames are already saved as aXXXX.jpg in a folder\n",
        "                          ## if True, then we need to preprocess first\n",
        "## Check data\n",
        "if config_PK.Data.raw_video_dir is not None:\n",
        "    print('[Process Raw Video]')\n",
        "    preprocess_video = True\n",
        "    data_dir = config_PK.Data.raw_video_dir\n",
        "    if isinstance(config_PK.Data.raw_video_dir, list) or isinstance(config_PK.Data.raw_video_dir, omegaconf.listconfig.ListConfig):\n",
        "        print(config_PK.Data.raw_video_dir)\n",
        "    else:\n",
        "        # raise NotImplementedError\n",
        "        assert os.path.exists(config_PK.Data.raw_video_dir)\n",
        "elif config_PK.Data.sonosite_frame_dir is not None:\n",
        "    print('[Process non-LCR Frames]')\n",
        "    data_dir = config_PK.Data.sonosite_frame_dir\n",
        "    assert os.path.exists(config_PK.Data.sonosite_frame_dir)\n",
        "elif config_PK.Data.prodigy_frame_dir is not None:\n",
        "    print('[Process LCR Frames]')\n",
        "    data_dir = config_PK.Data.prodigy_frame_dir\n",
        "    assert os.path.exists(config_PK.Data.prodigy_frame_dir+\"/L\")\n",
        "    assert os.path.exists(config_PK.Data.prodigy_frame_dir+\"/R\")\n",
        "    if not os.path.exists(config_PK.Data.prodigy_frame_dir+\"/C\"):\n",
        "        ## TODO get the middle frames\n",
        "        raise NotImplementedError\n",
        "\n",
        "save_json=config_PK.User_setting.save_json   ## save prediction needles coordinates to json\n",
        "save_mask=config_PK.User_setting.save_mask   ## save PK result plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set coordinates\n",
        "If input raw prodigy video, there are two options:\n",
        "1. Capture the coordinates on the first frame of the video: Run  the 1st cell below (`get_coord`) and make 3 clicks on the frame (clicking the upper-left of the 2 images and the bottom-right of the first image is enough). Then, manually set the coordinates of the 3 square area.\n",
        "2. Use the default coordinate setting of `家庭資料室_Developer/Prodigy管理/zipper array data for PK豬肉打針/ultrasound_2025-06-13-15-07.mp4`: Run the 2nd cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Optional: get_coord\n",
        "coords = []\n",
        "class GetCoords:\n",
        "    def __init__(self, frame1):\n",
        "        self.counter = 0\n",
        "        cv2.imshow(\"Image\", frame1)\n",
        "        cv2.setMouseCallback(\"Image\", self.click_event, frame1)\n",
        "        cv2.waitKey()\n",
        "    def click_event(self, event, x, y, flag, param):\n",
        "        if event == cv2.EVENT_LBUTTONDOWN:\n",
        "            coords.append((x, y))\n",
        "            print(f\"Clicked at: ({x}, {y})\")\n",
        "            # Optional: draw a circle\n",
        "            cv2.circle(param, (x, y), 5, (0, 255, 0), -1)\n",
        "            cv2.imshow(\"Image\", param)\n",
        "            self.counter += 1\n",
        "            if self.counter == 3:\n",
        "                print('Completed colllection of coords.')\n",
        "                cv2.destroyAllWindows()\n",
        "                return 0\n",
        "\n",
        "# Load first frame from video  ------------------\n",
        "cap = cv2.VideoCapture(config_PK.Data.raw_video_dir)\n",
        "assert cap.isOpened(), \"Error opening video file.\"\n",
        "while True:\n",
        "    ret, frame1 = cap.read()\n",
        "    break\n",
        "cap.release()\n",
        "# -------------------------------------------------\n",
        "\n",
        "## make some clicks to check the required coordinates\n",
        "getcoord = GetCoords(frame1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Manual coordinate setting (must check for raw prodigy video)\n",
        "## upper-right of the Left, Center, Right images\n",
        "h, w = config_PK.Data.frame_h, config_PK.Data.frame_w  ## image size\n",
        "y_up = 150\n",
        "x_l = 52\n",
        "x_c = x_l+w\n",
        "x_r = x_c+w\n",
        "\n",
        "## set the area of each image\n",
        "y_1, y_2 = y_up, y_up+h\n",
        "x_l1, x_l2 = x_l, x_l+w\n",
        "x_c1, x_c2 = x_c, x_c+w\n",
        "x_r1, x_r2 = x_r, x_r+w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Setup\n",
        "- Read config to build model\n",
        "- transform to tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @Model setting\n",
        "## Set Configuration from json\n",
        "if config_PK.Detection_model.name == \"m2f\":\n",
        "    with open(\"configs/config_m2f.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "    with open(\"./model/mask2former/m2f_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        m2f_config = json.load(f)\n",
        "    config = merge(config, m2f_config)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # Model Initialization\n",
        "    # --------------------------------------------------------------------------\n",
        "    if config[\"Model\"][\"unet_backbone\"][\"encoder_type\"] == \"TransNeXt-Tiny\":\n",
        "        model_name = \"TransNeXt-Mask2Former\"\n",
        "    elif config[\"Model\"][\"unet_backbone\"][\"encoder_type\"] == \"ConvNeXt\":\n",
        "        model_name = \"ConvNeXt-Mask2Former\"\n",
        "    \n",
        "    model = Mask2Former(config)\n",
        "    anchors_pos = None\n",
        "    det_head = False\n",
        "\n",
        "elif config_PK.Detection_model.name == \"memm2f\":\n",
        "    with open(\"configs/config_memm2f.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "    with open(\"./model/memmask2former/mem_m2f_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        m2f_config = json.load(f)\n",
        "    config = merge(config, m2f_config)\n",
        "    model_name = 'MemM2F'\n",
        "\n",
        "    ## recheck decoder input dimension\n",
        "    encoder_dim = {\"ConvNeXt\":[1024, 512, 256, 128], \"TransNeXt-Tiny\":[576, 288, 144, 72]}\n",
        "    if \"pixel\" in config[\"Model\"][\"unet_backbone\"][\"decoder_type\"]:\n",
        "        track_model_cfg.mask_decoder.up_dims = [256,256,256]\n",
        "        track_model_cfg.pixel_encoder.ms_dims = [256]*4\n",
        "        track_model_cfg.embed_dim = 256\n",
        "        if config[\"Model\"][\"unet_backbone\"][\"decoder_type\"] == \"pixelup\":\n",
        "            track_model_cfg.pixel_encoder.ms_dims[-1] = encoder_dim[config[\"Model\"][\"unet_backbone\"][\"encoder_type\"]][0]\n",
        "    else:\n",
        "        track_model_cfg.mask_decoder.up_dims[0] = encoder_dim[config[\"Model\"][\"unet_backbone\"][\"encoder_type\"]][0]\n",
        "        track_model_cfg.pixel_encoder.ms_dims = encoder_dim[config[\"Model\"][\"unet_backbone\"][\"encoder_type\"]]\n",
        "        track_model_cfg.embed_dim = encoder_dim[config[\"Model\"][\"unet_backbone\"][\"encoder_type\"]][0]\n",
        "\n",
        "    track_eval_cfg = OmegaConf.load(\"./model/memmask2former/eval_config.yaml\")\n",
        "    model = MemInferenceWrapper(cfg=config, track_model_cfg=track_model_cfg, \n",
        "                                    track_eval_cfg=track_eval_cfg)\n",
        "\n",
        "if config[\"Model\"].get(\"dynamic_tanh\"):\n",
        "    from model.utils import convert_ln_to_dyt\n",
        "    model = convert_ln_to_dyt('','',model)\n",
        "\n",
        "print('\\n[MODEL]:',model_name)\n",
        "\n",
        "## Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device_name = torch.cuda.get_device_name(device) if torch.cuda.is_available() else \"CPU\"\n",
        "print(f\"[DEVICE]: {device}\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Read ckpt\n",
        "# --------------------------------------------------------------------------\n",
        "ckpt_path = config[\"Model\"].get(\"ckpt_path\")\n",
        "assert os.path.exists(ckpt_path)\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cuda\")  ## , weights_only=False\n",
        "if \"n_averaged\" in ckpt.keys(): ## key names in ema_model & model are slightly different\n",
        "    ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(config[\"Validation\"][\"ema\"]))\n",
        "    model = ema_model\n",
        "model.load_state_dict(ckpt, strict=True)\n",
        "print(f\"[LOAD] Ckpt: {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @Data pre-processing\n",
        "valid_transform = Augmentation(crop=False, rotate=False, color_jitter=False, horizontal_flip=False, \n",
        "                               image_size=config[\"Model\"][\"image_size\"])\n",
        "trans_totensor = tf.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code from Radon_by_Jacky_no_PK\n",
        "- 參數設定\n",
        "- Flag \n",
        "- PK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cutNum = config_PK.PK_hyperparam.cutNum  ## maximum cuts (carriages) of the train\n",
        "left_bright_weight = config_PK.PK_hyperparam.left_bright_weight\n",
        "left_shadow_weight = config_PK.PK_hyperparam.left_shadow_weight\n",
        "right_bright_weight = config_PK.PK_hyperparam.right_bright_weight\n",
        "right_shadow_weight = config_PK.PK_hyperparam.right_shadow_weight\n",
        "\n",
        "width, height = config_PK.Data.frame_h, config_PK.Data.frame_w  ## If this is <= 0, the code will get the size from reading a frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEb3bus5hPiV"
      },
      "outputs": [],
      "source": [
        "class Flag:\n",
        "    def __init__(self, cutNum=32, height=1758, width=1758):\n",
        "        \n",
        "        self.needle_location = [0,0,0,0]  #[x1,y1,x3,y3]\n",
        "        self.height = height ## origin:1080\n",
        "        self.width = width  ## origin:1920\n",
        "        \n",
        "        self.filename=\"\"\n",
        "        self.save_folder_name = ''\n",
        "        self.found_needle = False  ## origin: hough_found_needle\n",
        "        self.count1=0\n",
        "        \n",
        "        #要跳過幾個frame數\n",
        "        self.run_only_one = False    #跑單張 frame\n",
        "        self.fps_reduce = 1\n",
        "\n",
        "        #火車(又名pk、梯度)\n",
        "        self.cutNum = cutNum\n",
        "        \n",
        "        self.pk_regression_box=[0,0]\n",
        "        self.pk_result_先左右互扣 = list(range(self.cutNum))  #保留\n",
        "        self.pk_result_再左右互扣 = list(range(self.cutNum))  #保留\n",
        "        \n",
        "        self.carriage_x = []  ## record [Xleft_point,Xright_point] for each carriage (the ROI bbox of the train)\n",
        "        self.carriage_y = list(range(self.cutNum)) ## record [Y_1_point,Y_0_point] for each carriage (the ROI bbox of the train)\n",
        "        \n",
        "        # ---------------------------\n",
        "\t\t# Set carriage coordinate (ROI bbox of each part of train)\n",
        "\t\t# ---------------------------\n",
        "        for i in range(0,cutNum):  \n",
        "            ## fixed x axis at each cut\n",
        "            Xleft_point = int(round( 0 + width*(i+0.1)/cutNum,0) )\n",
        "            Xright_point = int(round( 0 + width*(i+0.9)/cutNum,0) )\n",
        "            self.carriage_x.append([Xleft_point,Xright_point])\n",
        "\n",
        "        \n",
        "\t\t# region ## not used now\n",
        "    \t# self.save_ori_frame=False\n",
        "    \t# self.綠框_result = list(range(self.cutNum)) #保留 ## record flag.bright at each carriage\n",
        "        # self.綠框_score = 0.0\n",
        "        # self.綠框_variance = 0.0\n",
        "        # self.綠框_square = 0.0\n",
        "        # self.run_time_list=[]\n",
        "        # self.run_time_txt=[]\n",
        "        # self.find_train_threshold = 400\n",
        "        # self.pk_score = 0.0  ## R^2 score of regression\n",
        "        # self.pk_variance = 0.0\n",
        "        # self.left_b_minus_s = []\n",
        "        # self.right_b_minus_s = []\n",
        "        # #XY陣列找參數，目前關閉只取第一個，blur 5 deg 14\n",
        "        # self.blur = 3 # 7/3版是6 --> 4/16demo版本是3\n",
        "        # #JSON\n",
        "        # json_txt = '{}'\n",
        "        # self.json_obj = json.loads(json_txt)\n",
        "\t\t# endregion\n",
        "\n",
        "\n",
        "    def pk_regression(self):\n",
        "        '''\n",
        "        LinearRegression to predict the depth at each carriage.\n",
        "        Store prediction in self.pk_regression_box_再左右互扣\n",
        "        '''\n",
        "        # try:\n",
        "        #x = np.array(list(range(self.cutNum))).reshape((-1, 1))\n",
        "        x = np.array(list(range(self.find_train_end_right - self.find_train_end_left +1))).reshape((-1, 1))\n",
        "        #print(\"x=\",x.shape)\n",
        "        #print(\"flag.find_train_end_right,flag.find_train_end_left=\",flag.find_train_end_right, flag.find_train_end_left)\n",
        "        #print(\"self.pk_result=\",self.pk_result_先左右互扣)\n",
        "        y = np.array(self.pk_result_先左右互扣[self.find_train_end_left:self.find_train_end_right+1])\n",
        "        # print('[y]', y)\n",
        "        with parallel_backend('threading', n_jobs=-2):  ## n_cpu +1 +n_jobs are used for parallelizing \n",
        "            model = LinearRegression().fit(x,y)\n",
        "            self.pk_regression_box_先左右互扣 = model.predict(x).tolist()\n",
        "\n",
        "        y = np.array(self.pk_result_再左右互扣[self.find_train_end_left:self.find_train_end_right+1])\n",
        "        with parallel_backend('threading', n_jobs=-2):  ## n_cpu +1 +n_jobs are used for parallelizing \n",
        "            model = LinearRegression().fit(x,y)\n",
        "            self.pk_regression_box_再左右互扣 = model.predict(x).tolist()  ## predicted depth\n",
        "            # self.pk_score = model.score(x, y)\n",
        "        # print('[self.pk_regression_box_再左右互扣]', self.pk_regression_box_再左右互扣)\n",
        "\n",
        "        # print(\"[explained_variance_score] pk_regression_box=\",self.pk_regression_box, 'y', y)\n",
        "        # self.pk_variance = explained_variance_score(y, self.pk_regression_box)  ## error here but not sure what this is for\n",
        "        # except:\n",
        "        #     print(\"pk_regression有問題\")\n",
        "        #     pass\n",
        "\n",
        "\t# region ## not used now\n",
        "    def rm_dummy(self,img):\n",
        "      from skimage.transform import resize\n",
        "\n",
        "      if 判斷是否為RGB(img):\n",
        "        height, width, __  = img.shape\n",
        "      else:\n",
        "        height, width  = img.shape\n",
        "      #print(\"before rm_dummy img=\",img.shape)\n",
        "      img_new = img[int(round(width/2 - width/1.41421356/2)):int(round(width/2 + width/1.41421356/2)), int(round(width/2 - width/1.41421356/2)):int(round(width/2 + width/1.41421356/2))]\n",
        "      #print(\"after rm_dummy img=\",img_new.shape)\n",
        "\n",
        "      return img_new.astype(np.float32)\n",
        "\n",
        "    def run_time(self):\n",
        "      #print(self.run_time_list)\n",
        "      #print(self.run_time_txt)\n",
        "      run_time_list_new = [None] * (len(self.run_time_list)-1)\n",
        "      for i in range(1,len( self.run_time_list)):\n",
        "        #print(\"run_time_list[i].microsecond=\", self.run_time_list[i].timestamp() )\n",
        "        run_time_list_new[i-1] = self.run_time_list[i].timestamp() - self.run_time_list[i-1].timestamp()\n",
        "      #print( run_time_list_new)\n",
        "      fig , ax = plt.subplots()\n",
        "      ax.pie(run_time_list_new ,labels = self.run_time_txt[1:len( self.run_time_txt)], autopct=\"%.0f%%\")\n",
        "      ax.set_title('Total time: '+str( self.run_time_list[len( self.run_time_list)-1].timestamp() - self.run_time_list[0].timestamp()), fontsize=16)\n",
        "\n",
        "    def save_func(self,j):\n",
        "      self.save_folder_name = str(\"/zip2(202201月6mm 225075)autolable/theta105(20)_sinogram(sobel03x07y185_1012subrectangle075_140zero_HoughR1L40G20)train(i+2)\")\n",
        "      import shutil\n",
        "      try:\n",
        "        ################注意：清除舊的遠端\n",
        "        shutil.rmtree( \"/content/gdrive/MyDrive/cloud_film\" + self.save_folder_name + \"_\" + str(j))\n",
        "        #pass\n",
        "      except:\n",
        "        pass\n",
        "      shutil.copytree('/content/dataset_auto', '/content/gdrive/MyDrive/cloud_film' + self.save_folder_name + \"_\" + str(j), symlinks = False, ignore = None)\n",
        "\n",
        "      ################注意：清除舊的本地\n",
        "      try:\n",
        "        shutil.rmtree(\"/content/dataset_auto/\")\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    def show_3D(self, filename):\n",
        "      import matplotlib.pyplot as plt\n",
        "      #from mpl_toolkits.mplot3d import Axes3D\n",
        "      import numpy as np\n",
        "\n",
        "      if self.found_needle:\n",
        "        size_3D = 2\n",
        "        x, y, z = np.indices((100*size_3D, 100*size_3D, 100*size_3D))\n",
        "        cube_left = (70*size_3D >= x) & (x >= 30*size_3D) & (y == 52*size_3D) & (60*size_3D >= z) & (z >= 20*size_3D)\n",
        "        cube_right = (70*size_3D >= x) & (x >= 30*size_3D) & (y == 48*size_3D) & (60*size_3D >= z) & (z >= 20*size_3D)\n",
        "\n",
        "        self.json_obj[\"pk_regression_box\"] = self.pk_regression_box\n",
        "        #flag.json_obj[\"needle_location\"] = flag.needle_location\n",
        "        #flag.json_obj[\"綠框_regression_box\"] = flag.綠框_regression_box\n",
        "        self.json_obj[\"needle_location\"] = self.needle_location\n",
        "\n",
        "        #'''\n",
        "        #用 houghP 來算 3D 圖 XY\n",
        "        x1 = self.needle_location[0]\n",
        "        y1 = self.needle_location[1]\n",
        "        x3 = self.needle_location[2]\n",
        "        y3 = self.needle_location[3]\n",
        "        needle_width = x3 - x1\n",
        "        needle_height = y3 - y1\n",
        "\n",
        "        Y_middle = (-(y1+y3) / 2 + (self.height /2))/100 *size_3D\n",
        "        #print(\"Z_middle=\",Z_middle)\n",
        "\n",
        "        #用 綠框_regression 來算 3D 圖 Z\n",
        "        #Z_deg = -(self.pk_regression_box_再左右互扣[-1] - self.pk_regression_box_再左右互扣[0])/200\n",
        "        Z_deg = -(self.pk_regression_box_再左右互扣[-1] - self.pk_regression_box_再左右互扣[0])/300\n",
        "\n",
        "        print(\"Z_deg=\",Z_deg)\n",
        "        X_middle = ((x1+x3)/2 - self.width/4)/50 *size_3D\n",
        "\n",
        "        #Z_middle = (self.綠框_regression_box[-1] + self.綠框_regression_box[0])/-10 *size_3D  #0619 Y太出去了\n",
        "        Z_middle = (self.pk_regression_box_再左右互扣[-1] + self.pk_regression_box_再左右互扣[0])/-10 *size_3D  #0619 Y太出去了\n",
        "        print(\"Z_middle=\",Z_middle)\n",
        "\n",
        "        #print(\"(y1+y3)/2=\",(y1+y3)/2)\n",
        "        #print(\"Y_middle=\",Y_middle)\n",
        "        #print(\"XY_deg=\",XY_deg)\n",
        "\n",
        "        try:\n",
        "          #slope = 200/needle_width\n",
        "          #print( slope,\"*x + \",flag.pk_regression_box[0]/5,\"+50 -y\")\n",
        "          #needle = ( abs(slope*(x-50) + 50 -y) <=2 ) & (z == 50)\n",
        "          #print(\"needle_height / needle_width=\",needle_height / needle_width)\n",
        "          XY_deg = needle_height / needle_width / -2\n",
        "          #print(\"Z_deg=\",Z_deg)\n",
        "          #needle = ( abs(XY_deg*(x - 50*size_3D -X_middle) + 50*size_3D + Y_middle -y) <=2 ) & ( abs(Z_deg*(x - 40*size_3D - X_middle) + 50*size_3D + Z_middle - z) <= 1)\n",
        "          needle = ( abs(Z_deg*(x - 50*size_3D -X_middle) + 50*size_3D + Z_middle -y) <=2 ) & ( abs(XY_deg*(x - 40*size_3D - X_middle) + 45*size_3D + Y_middle - z) <= 1) #0619 Z太高了\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        try:\n",
        "          if self.found_needle == False:\n",
        "            voxelarray = cube_left | cube_right\n",
        "          else:\n",
        "            voxelarray = cube_left | cube_right | needle\n",
        "          colors = np.empty(voxelarray.shape, dtype=object)\n",
        "          colors[needle] = 'red'\n",
        "        except:\n",
        "          voxelarray = cube_left | cube_right\n",
        "          colors = np.empty(voxelarray.shape, dtype=object)\n",
        "\n",
        "        colors[cube_left] = 'blue'\n",
        "        colors[cube_right] = 'green'\n",
        "        fig = plt.figure(figsize=(15,15))\n",
        "        #fig = plt.figure(figsize=(5,5))\n",
        "        ax = fig.add_subplot(projection='3d')\n",
        "        ax.voxels(voxelarray, facecolors=colors)\n",
        "\n",
        "        ax.set_axis_off()\n",
        "        #angle=90\n",
        "        angle=30\n",
        "        plt.gca().invert_xaxis()\n",
        "        #ax.view_init(30, angle)\n",
        "        #ax.view_init(20, angle)\n",
        "        ax.azim = 30\n",
        "        ax.dist = 10\n",
        "        ax.elev = 20\n",
        "        fig.savefig(filename)\n",
        "        plt.close()\n",
        "        fig.clear()\n",
        "\n",
        "        show_3D_img = cv2.imread(filename)\n",
        "        show_3D_img = self.rm_dummy(show_3D_img)\n",
        "        show_3D_img = self.rm_dummy(show_3D_img)\n",
        "        show_3D_img = self.rm_dummy(show_3D_img)\n",
        "        show_3D_img = self.rm_dummy(show_3D_img)\n",
        "        cv2.imwrite(filename, show_3D_img, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "\n",
        "        #'''\n",
        "      else:\n",
        "        shutil.copy('/content/img0000_3D_blank.jpg',filename)\n",
        "\t# endregion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_train_end_idx(flag):\n",
        "    x1 = flag.needle_location[0]\n",
        "    y1 = flag.needle_location[1]\n",
        "    x3 = flag.needle_location[2]\n",
        "    y3 = flag.needle_location[3]\n",
        "\n",
        "    ## the closest carriage that match the prediction endpoints\n",
        "    flag.x1_cut_idx = int(round(x1*flag.cutNum/flag.width - 0.5,0))\n",
        "    flag.x3_cut_idx = int(round(x3*flag.cutNum/flag.width - 0.5,0))\n",
        "    ## deal with too short needle\n",
        "    if flag.x3_cut_idx == flag.x1_cut_idx:\n",
        "        if flag.x3_cut_idx == flag.cutNum-1:  ## right side\n",
        "            flag.x1_cut_idx -= 1\n",
        "        elif flag.x3_cut_idx == 0: ## left side\n",
        "            flag.x3_cut_idx += 1\n",
        "        elif y3 > y1:\n",
        "            flag.x3_cut_idx += 1\n",
        "        else:\n",
        "            flag.x1_cut_idx += 1\n",
        "    \n",
        "    flag.find_train_end_left = min(flag.x1_cut_idx, flag.x3_cut_idx)\n",
        "    flag.find_train_end_right = max(flag.x1_cut_idx, flag.x3_cut_idx)\n",
        "    # print(f'[left right end] {flag.find_train_end_left, flag.find_train_end_right}')\n",
        "    return\n",
        "\n",
        "# region ## not used now\n",
        "def set_train_expand_end_idx(flag, img, find_train_center_i):\n",
        "    x1 = flag.needle_location[0]\n",
        "    x3 = flag.needle_location[2]\n",
        "    ################################### 往左延伸 find_train_end_left = i #############################################\n",
        "    flag.find_train_end_left = 0\n",
        "    for i in range(find_train_center_i, 0, -1):               #find_train_center_i = i\n",
        "        Xleft_point,Xright_point = flag.carriage_x[i]\n",
        "        Y_1_point,Y_0_point = flag.carriage_y[i]\n",
        "        # Y_1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 - 13) )\n",
        "        # Y_0_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1) + 13)\n",
        "        #Y_m1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width/2)/flag.cutNum*i - needle_height/flag.cutNum*1 + 200) )\n",
        "        #火車、線段檢查，用不到Y_m1_point。    pk才需要Y_m1_point去畫陰影區域  這裡的Y_m1_point是畫圖展現pk陰影法用的\n",
        "\n",
        "        #接下來算差值取平方\n",
        "        ################################### A.Needle location內 判斷是否為針​(有無黑點)\n",
        "        if Xleft_point > min(x1,x3):      # >x1 or >x1_ 才對，變數是否有更新過？\n",
        "            flag.find_train_threshold = 1200\n",
        "            if (( flag.綠框_result[i] - flag.綠框_result[i+1] ) ** 2 >= flag.find_train_threshold       #i, i+1\n",
        "                and flag.綠框_result[i] < 30 ) \\\n",
        "                or (( flag.綠框_result[i] - flag.綠框_result[i+2] ) ** 2 >= 3.6 * flag.find_train_threshold       #i, i+2,   若綠框是等差級數，應該是4倍threshold\n",
        "                and flag.綠框_result[i] < 30 ) \\\n",
        "                or flag.綠框_result[i] < 16:                                  #綠框_result[i]平均象素亮度會與Y point高度範圍有關\n",
        "                #if (A&A) or C\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (255, 0, 0), 2)\n",
        "                flag.find_train_end_left = i\n",
        "                #加上這不是針 needleOrFake=false, 不啟動pk\n",
        "                # print(f'[{Xleft_point} > x1] [R cv2.rectangle] blue')\n",
        "                break\n",
        "            else:\n",
        "                #在center圖上畫畫\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (0, 0, 255), 2)\n",
        "                # print(f'[{Xleft_point} > x1] [R cv2.rectangle] red')\n",
        "        ################################### #B.Needle location外 是針就寬容找端點\n",
        "        else:\n",
        "            flag.find_train_threshold = 800\n",
        "            if (( flag.綠框_result[i] - flag.綠框_result[i+1] ) ** 2 >= flag.find_train_threshold        #i, i+1\n",
        "                and flag.綠框_result[i] < 60 ) \\\n",
        "                or flag.綠框_result[i] <= 30:\n",
        "                #if (A&A) or C\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (18, 153, 255), 2) #BGR 橙色\n",
        "                flag.find_train_end_left = i\n",
        "                # print(f'[{Xleft_point} > x1] [R cv2.rectangle] org')\n",
        "                break\n",
        "            else:\n",
        "                #在center圖上畫畫\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (0, 255, 255), 2)\n",
        "                # print(f'[{Xleft_point} <= x1] [R cv2.rectangle] yellow')\n",
        "\n",
        "\n",
        "    ################################### 往右延伸 flag.find_train_end_right = i #############################################\n",
        "    flag.find_train_end_right = flag.cutNum\n",
        "    for i in range(find_train_center_i, flag.cutNum):           #find_train_center_i = i\n",
        "        Xleft_point,Xright_point = flag.carriage_x[i]\n",
        "        Y_1_point,Y_0_point = flag.carriage_y[i]\n",
        "        # Y_1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 - 13) )\n",
        "        # Y_0_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1) + 13 )\n",
        "        #Y_m1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width/2)/flag.cutNum*i - needle_height/flag.cutNum*1 + 200) )\n",
        "\n",
        "        #接下來算差值取平方\n",
        "        ################################### A.Needle location內 判斷是否為針​\n",
        "        if Xright_point < max(x1,x3): # <x3 or <x3_ 才對，變數是否有更新過？\n",
        "            flag.find_train_threshold = 1200\n",
        "            if (( flag.綠框_result[i] - flag.綠框_result[i-1] ) ** 2 >= flag.find_train_threshold      #i, i-1\n",
        "                and flag.綠框_result[i] < 30 ) \\\n",
        "                or (( flag.綠框_result[i] - flag.綠框_result[i-2] ) ** 2 >= 3.6 * flag.find_train_threshold      #i, i-2,   若綠框是等差級數，應該是4倍threshold\n",
        "                and flag.綠框_result[i] < 30 ) \\\n",
        "                or flag.綠框_result[i] < 16:\n",
        "                #if (A&A) or C\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (255, 0, 0), 2)  #BGR 藍色\n",
        "                flag.find_train_end_right = i\n",
        "                #加上這不是針 needleOrFake=false, 不啟動pk\n",
        "                break\n",
        "            else:\n",
        "                #在center圖上畫畫\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (0, 0, 255), 2) #BGR 紅色\n",
        "        ################################### #B.Needle location外 是針就寬容找端點\n",
        "        else:\n",
        "            flag.find_train_threshold = 800\n",
        "            if (( flag.綠框_result[i] - flag.綠框_result[i-1] ) ** 2 >= flag.find_train_threshold     #i, i-1\n",
        "                and flag.綠框_result[i] < 60 ) \\\n",
        "                or flag.綠框_result[i] <= 30:\n",
        "                #if (A&A) or C\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (18, 153, 255), 2) #BGR 橙色\n",
        "                flag.find_train_end_right = i\n",
        "                break\n",
        "            else:\n",
        "                #在center圖上畫畫\n",
        "                cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (0, 255, 255), 2) #BGR 黃色\n",
        "\n",
        "    return img\n",
        "\n",
        "def 線段檢查(flag, img, expand_len=False):\n",
        "    \"\"\"\n",
        "    Set the find_train_end_left & find_train_end_right & brightness of each carriage\n",
        "    - Input: \n",
        "        - flag\n",
        "        - current_frame_center ((L+R)/2)\n",
        "        - expand_len: Expand the length of train by pixel average value change.\n",
        "                        If false, directly use model prediction as the 2 ends of the train. \n",
        "    - Return:\n",
        "        - OutputImg_線段檢查: show where find_train_end_left & find_train_end_right are if expand_len\n",
        "    \"\"\"\n",
        "\n",
        "    x1 = flag.needle_location[0]\n",
        "    y1 = flag.needle_location[1]\n",
        "    x3 = flag.needle_location[2]\n",
        "    y3 = flag.needle_location[3]\n",
        "    needle_width = x3 - x1  #此二行是否可負值？yes\n",
        "    needle_height = y3 - y1\n",
        "    find_train_center = False\n",
        "\n",
        "    # ---------------------------\n",
        "    # Set the carriage idx of train start & end\n",
        "    # ---------------------------\n",
        "    set_train_end_idx(flag)\n",
        "    \n",
        "    # print(f\"[線段檢查] 畫出柱狀體 (x1,y1),(x3,y3) {(x1, y1)}{(x3, y3)}\")\n",
        "    # 火車來了 在原圖上畫畫(線段檢查先一律算出0-31，火車要正確端點)\n",
        "    temp = y1 - needle_height/needle_width*x1 - needle_height/flag.cutNum*1\n",
        "    for i in range(flag.find_train_end_left, flag.find_train_end_right+1):\n",
        "        # ---------------------------\n",
        "        # Carriage coordinate (ROI bbox of each part of train)\n",
        "        # ---------------------------\n",
        "        #Y_1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 + needle_height/flag.cutNum*4) )\n",
        "        #固定Y1=Y0+15 pixel，後方的Y_1_point也都一併修改\n",
        "        Y_1_point = int(round( temp + needle_height/needle_width*(flag.width)/flag.cutNum*i - 13) )\n",
        "        Y_0_point = int(round( temp + needle_height/needle_width*(flag.width)/flag.cutNum*i) + 13 )\n",
        "        \n",
        "        #Y_m1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 + 200) )\n",
        "        #火車、線段檢查，用不到Y_m1_point。    pk才需要Y_m1_point去畫陰影區域 這裡的Y_m1_point是畫圖展現pk陰影法用的\n",
        "\n",
        "        flag.carriage_y[i] = [Y_1_point,Y_0_point] ###\n",
        "\n",
        "        if expand_len: ## default False\n",
        "            Xleft_point, Xright_point = flag.carriage_x[i]\n",
        "            # ---------------------------\n",
        "            #某一節車廂的平均亮度\n",
        "            # ---------------------------\n",
        "            temp_area = img[Y_1_point:Y_0_point, Xleft_point: Xright_point]\n",
        "            bright = np.average(temp_area)\n",
        "            # if np.isnan(bright):\n",
        "            #    print('[Warn][line check] bright is nan, convert to 0')\n",
        "            #    bright = 0\n",
        "            #未來可以把pk串在這後面，以免重複計算亮區。不過，線段檢查火車是算0-31的亮區，pk只需要特定eg.4-18亮區(Y_1_point)，以及下方暗區(Y_m1_point)。\n",
        "            #用get_result,把sum/height=result_bright, result_shadow, return result_bright/2, result_bright - result_shadow\n",
        "            flag.綠框_result[i] = bright *255  ## only used for expand_len \n",
        "            \n",
        "            # ---------------------------\n",
        "            #算出火車中心點\n",
        "            # ---------------------------\n",
        "            if Xleft_point > (x3+x1)/2 and find_train_center == False:   #某節車廂的左界>needle location中點則停下來find_train_center_i = i\n",
        "                find_train_center = True\n",
        "                find_train_center_i = i\n",
        "\n",
        "    if expand_len:  ## default False\n",
        "        # ---------------------------\n",
        "        # Set the carriage idx of train start & end by expanding from center\n",
        "        # ---------------------------\n",
        "        img = set_train_expand_end_idx(flag, img, find_train_center_i)\n",
        "\n",
        "    # #################################################################畫底線在 960 上面 (1920/2)\n",
        "    # OutputImg_green = np.zeros((int(round(flag.height/4)), int(round(flag.width))), dtype=np.float32)\n",
        "    # OutputImg_green = cv2.cvtColor(OutputImg_green, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # # print(\"線段檢查 [flag.find_train_end_left, find_train_end_right] \",flag.find_train_end_left,flag.find_train_end_right)\n",
        "    # #for i in range(flag.find_train_end_left, flag.find_train_end_right):\n",
        "    # for i in range(0, flag.cutNum):\n",
        "    #     Xleft_point,Xright_point = flag.carriage_x[i]\n",
        "    #     cv2.rectangle(OutputImg_green, (int(round(Xleft_point)),int(round(flag.height/8)) ), (int(round(Xright_point)), int(round(flag.height/8+flag.綠框_result[i]*-2 )) ), (255, 255, 255), -1)\n",
        "    \n",
        "    return img #, OutputImg_green.astype(np.float32)\n",
        "# endregion\n",
        "\n",
        "#左右二張圖 OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣 = PK(flag, current_frame.copy())\n",
        "def PK(flag, img, return_plot=False, set_train_idx_point=False):\n",
        "    \"\"\"\n",
        "    Compare the left & right frame.\n",
        "    Originally,\n",
        "    - Input:\n",
        "        - flag\n",
        "        - img: left & right image concat together\n",
        "    - Return: \n",
        "        - OutputImg_PK, \n",
        "        - OutputImg_pk_regression_先左右互扣\n",
        "        - OutputImg_pk_regression_再左右互扣\n",
        "    \"\"\"\n",
        "    # print(\"[PK] 畫出柱狀體\")\n",
        "    x1 = flag.needle_location[0]\n",
        "    y1 = flag.needle_location[1]\n",
        "    x3 = flag.needle_location[2]\n",
        "    y3 = flag.needle_location[3]\n",
        "    needle_width = x3 - x1  #此二行是否可負值？\n",
        "    needle_height = y3 - y1\n",
        "\n",
        "    if set_train_idx_point:\n",
        "        # ---------------------------\n",
        "        # Set the carriage idx of train start & end\n",
        "        # ---------------------------\n",
        "        set_train_end_idx(flag)\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    #for i in range(0,flag.cutNum):\n",
        "    for i in range(flag.find_train_end_left, flag.find_train_end_right+1):\n",
        "        #Y_1_point = int(round( y1 + needle_height/flag.cutNum*i - needle_height/flag.cutNum*1 + needle_height/flag.cutNum*4) )\n",
        "        #Y_0_point = int(round( y1 + needle_height/flag.cutNum*i - needle_height/flag.cutNum*1) )\n",
        "        #Y_m1_point = int(round( y1 + needle_height/flag.cutNum*i - needle_height/flag.cutNum*1 - needle_height/flag.cutNum*32))\n",
        "        Xleft_point,Xright_point = flag.carriage_x[i]\n",
        "        \n",
        "        Y_m1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 + 200) )\n",
        "        if set_train_idx_point:\n",
        "            Y_1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 - 13) )\n",
        "            Y_0_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1) + 10 )\n",
        "            flag.carriage_y[i] = [Y_1_point,Y_0_point]\n",
        "        else:\n",
        "            Y_1_point, Y_0_point = flag.carriage_y[i]\n",
        "            Y_0_point = Y_0_point - 3\n",
        "\n",
        "        #先bright-shadow，再左右互扣 (same result as 先左右互扣)\n",
        "        # ---------------------------\n",
        "        # needle bright area (L) - bottom shadow area (L)\n",
        "        # ---------------------------\n",
        "        ###################left\n",
        "        #########################紅框pk亮度(亮區佔70%pk)\n",
        "        temp_area = img[Y_1_point:Y_0_point, Xleft_point: Xright_point]  ## LB\n",
        "        bright = np.average(temp_area)\n",
        "        \n",
        "        #########################白框pk暗度(暗區佔30%pk)\n",
        "        temp_area = img[Y_0_point:Y_m1_point, Xleft_point: Xright_point]  ## LS\n",
        "        shadow = np.average(temp_area)\n",
        "        ###################亮度-暗度\n",
        "        ## 0.7LB - 0.3LS\n",
        "        left_result = (left_bright_weight*bright - left_shadow_weight*shadow) / 1  #值太大，人工縮小一點\n",
        "\n",
        "        # ---------------------------\n",
        "        # needle bright area (R) - bottom shadow area (R)\n",
        "        # ---------------------------\n",
        "        ###################right\n",
        "        #########################紅框pk亮度(亮區佔70%pk)\n",
        "        temp_area = img[Y_1_point:Y_0_point, Xleft_point+int(round(flag.width)): Xright_point+int(round(flag.width))]  ## RB\n",
        "        bright = np.average(temp_area)\n",
        "        #########################白框pk暗度(暗區佔30%pk)\n",
        "        temp_area = img[Y_0_point:Y_m1_point, Xleft_point+int(round(flag.width)): Xright_point+int(round(flag.width))]  ## RS\n",
        "        shadow = np.average(temp_area)\n",
        "        ###################亮度-暗度\n",
        "        ## 0.7RB - 0.3RS\n",
        "        right_result = (right_bright_weight*bright - right_shadow_weight*shadow) / 1   #值太大，人工縮小一點\n",
        "\n",
        "        # ##############################左右相減亮為正 左右相減暗為正 (之前是0.7bright-0.3shadow，再左右互扣。目前是先左右互扣，再bright-shadow)\n",
        "        # ## 0.7LB - 0.3LS - 0.7RB + 0.3RS\n",
        "        flag.pk_result_再左右互扣[i] = (left_result - right_result) *255\n",
        "        flag.pk_result_先左右互扣[i] = (left_result - right_result) *255\n",
        "\n",
        "        if return_plot:\n",
        "            #在原圖上畫畫\n",
        "            #left\n",
        "            cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_1_point), (0, 0, 255), 2)\n",
        "            cv2.rectangle(img, (Xleft_point,Y_0_point), (Xright_point, Y_m1_point), (255, 255, 255), 2)   #mark掉，就可以變透明的\n",
        "            #right\n",
        "            cv2.rectangle(img, (Xleft_point+int(round(flag.width)),Y_0_point), (Xright_point+int(round(flag.width)), Y_1_point), (0, 0, 255), 2)\n",
        "            cv2.rectangle(img, (Xleft_point+int(round(flag.width)),Y_0_point), (Xright_point+int(round(flag.width)), Y_m1_point), (255, 255, 255), 2)   #mark掉，就可以變透明的\n",
        "\n",
        "\n",
        "    #要等前面算完一次，再畫出regression = y1_regression, y3_regression\n",
        "    flag.pk_regression()\n",
        "    ################################################################# PK #########################################\n",
        "    #畫底線在 960 上面 (1920/2)\n",
        "    OutputImg_pk_regression_先左右互扣 = np.zeros((int(round(flag.height/4)), int(round(flag.width))), dtype=np.float32)\n",
        "    OutputImg_pk_regression_先左右互扣 = cv2.cvtColor(OutputImg_pk_regression_先左右互扣, cv2.COLOR_GRAY2RGB)\n",
        "    OutputImg_pk_regression_再左右互扣 = OutputImg_pk_regression_先左右互扣.copy()\n",
        "\n",
        "    if return_plot:\n",
        "        j=0\n",
        "        for i in range(flag.find_train_end_left, flag.find_train_end_right+1):\n",
        "            Xleft_point,Xright_point = flag.carriage_x[i]\n",
        "            Y_1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 - 13) )\n",
        "            Y_0_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1) + 13 )\n",
        "            Y_m1_point = int(round( y1-needle_height/needle_width*x1 + needle_height/needle_width*(flag.width)/flag.cutNum*i - needle_height/flag.cutNum*1 + 200) )\n",
        "            #   print(\"flag.pk_result_先左右互扣[i]=\",flag.pk_result_先左右互扣[i])\n",
        "            cv2.rectangle(OutputImg_pk_regression_先左右互扣, (int(round(Xleft_point)),int(round(flag.height/8)) ), (int(round(Xright_point)), int(round(flag.height/8+flag.pk_result_先左右互扣[i]*-2 )) ), (255, 255, 255), -1)\n",
        "            #   print(\"flag.pk_result_再左右互扣[i]=\",flag.pk_result_再左右互扣[i])\n",
        "            cv2.rectangle(OutputImg_pk_regression_再左右互扣, (int(round(Xleft_point)),int(round(flag.height/8)) ), (int(round(Xright_point)), int(round(flag.height/8+flag.pk_result_再左右互扣[i]*-2 )) ), (255, 255, 255), -1)\n",
        "            j=j+1\n",
        "\n",
        "        # try:\n",
        "        #   #cv2.line(OutputImg_PK, (0,int(round(flag.height/8 + flag.pk_regression_box[0]*-3 )) ), (int(round(Xright_point)), int(round(flag.height/8 + flag.pk_regression_box[-1]*-3 ))), (0,0,255), 20, cv2.LINE_AA)\n",
        "        cv2.line(OutputImg_pk_regression_先左右互扣, \n",
        "                (int(round(0 + flag.width*(flag.find_train_end_left+0.1)/flag.cutNum,0 )) ,\n",
        "                int(round(flag.height/8 + flag.pk_regression_box_先左右互扣[0]*-3 )) ),   \n",
        "                (int(round(Xright_point)), \n",
        "                int(round(flag.height/8 + flag.pk_regression_box_先左右互扣[-1]*-3 ))), (0,0,255), 20, cv2.LINE_AA)\n",
        "        cv2.line(OutputImg_pk_regression_再左右互扣, \n",
        "                (int(round(0 + flag.width*(flag.find_train_end_left+0.1)/flag.cutNum,0 )) ,\n",
        "                int(round(flag.height/8 + flag.pk_regression_box_再左右互扣[0]*-3 )) ), \n",
        "                (int(round(Xright_point)), \n",
        "                    int(round(flag.height/8 + flag.pk_regression_box_再左右互扣[-1]*-3 ))), (0,0,255), 20, cv2.LINE_AA)\n",
        "        # except:\n",
        "        #   pass\n",
        "\n",
        "    return img.astype(np.float32), OutputImg_pk_regression_先左右互扣.astype(np.float32), OutputImg_pk_regression_再左右互扣\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other Utils\n",
        "- Plot\n",
        "- Postprocessing\n",
        "- Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_pred_to_plot(image_C, pred_mask, pred_line, gt_mask, gt_line, \n",
        "              OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣,\n",
        "              fname=\"pred.png\"):\n",
        "    \"\"\"\n",
        "    Plot model prediction (if prediction is not None) & PK result\n",
        "    - First title: the coordinates\n",
        "    - Row 1: Center image with prediction dot & pred mask\n",
        "    - Row 2~4: image PK on Left & Right images\n",
        "    \"\"\"\n",
        "    ## do not save image if no pred\n",
        "    if len(np.unique(pred_mask)) <= 1:\n",
        "        return\n",
        "    row, col, scale = 4, 1, 3\n",
        "    fig, axes = plt.subplots(row, col, gridspec_kw={'height_ratios': [3, 3, 1,1]}, figsize=(3*col*scale, row*scale))\n",
        "\n",
        "    ## Row 1: image with prediction dot & pred mask\n",
        "    image_pred = np.concatenate([image_C, pred_mask], axis =-1)\n",
        "    axes[0].imshow((image_pred * 255).astype(np.uint8), cmap='gray')\n",
        "    if pred_line is not None and len(np.unique(pred_mask)) > 1:\n",
        "        axes[0].scatter([pred_line[0], pred_line[2]], [pred_line[1], pred_line[3]], c='red', s=10, label='pred')  # Plot dots\n",
        "        axes[0].set_title(', '.join(str(int(x)) for x in pred_line))\n",
        "    if gt_line is not None:\n",
        "        axes[0].scatter([gt_line[0], gt_line[2]], [gt_line[1], gt_line[3]], c='green', s=10, label='gt')  # Plot dots\n",
        "\n",
        "    ## Row 2~4: image PK on Left & Right\n",
        "    cv2.putText(OutputImg_PK,\"10\",(0,int(round(OutputImg_PK.shape[0]*7/8)) ) ,cv2.FONT_HERSHEY_SIMPLEX,2, (255, 255, 255), 8, cv2.LINE_AA)\n",
        "    cv2.putText(OutputImg_pk_regression_先左右互扣,\"10-1:first\",(0,int(round(OutputImg_pk_regression_先左右互扣.shape[0]*7/8)) ) ,cv2.FONT_HERSHEY_SIMPLEX,2, (255, 255, 255), 8, cv2.LINE_AA)\n",
        "    cv2.putText(OutputImg_pk_regression_再左右互扣,\"10-2:after\",(0,int(round(OutputImg_pk_regression_再左右互扣.shape[0]*7/8)) ) ,cv2.FONT_HERSHEY_SIMPLEX,2, (255, 255, 255), 8, cv2.LINE_AA)\n",
        "    if OutputImg_PK is not None:\n",
        "        # OutputImg_線段檢查 = cv2.cvtColor(OutputImg_線段檢查, cv2.COLOR_BGR2RGB)\n",
        "        OutputImg_PK = cv2.cvtColor(OutputImg_PK, cv2.COLOR_BGR2RGB)\n",
        "        OutputImg_pk_regression_先左右互扣 = cv2.cvtColor(OutputImg_pk_regression_先左右互扣, cv2.COLOR_BGR2RGB)\n",
        "        OutputImg_pk_regression_再左右互扣 = cv2.cvtColor(OutputImg_pk_regression_再左右互扣, cv2.COLOR_BGR2RGB)\n",
        "        # axes[1].imshow(OutputImg_線段檢查[:1759,:1759].astype('uint8'))\n",
        "        axes[1].imshow(OutputImg_PK.astype('uint8'))\n",
        "        axes[2].imshow(OutputImg_pk_regression_先左右互扣.astype('uint8'))\n",
        "        axes[3].imshow(OutputImg_pk_regression_再左右互扣.astype('uint8'))\n",
        "        axes[1].set_title('OutputImg_PK')\n",
        "        axes[2].set_title('OutputImg_pk_first')\n",
        "        axes[3].set_title('OutputImg_pk_after')\n",
        "    \n",
        "    for r in range(row):\n",
        "        axes[r].axis('off')\n",
        "    plt.margins(0,0)\n",
        "    # plt.show()\n",
        "    plt.savefig(fname, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_pred_to_json(fnames, pred_coord_label, img_size, save_folder_dir=None):\n",
        "    if save_folder_dir is not None:\n",
        "        filename = os.path.basename(fnames[0]).replace(\".jpg\", \".json\")\n",
        "        json_path = os.path.join(save_folder_dir, filename)\n",
        "    else:\n",
        "        json_path = fnames[0].replace(\".jpg\", \".json\")\n",
        "    if os.path.exists(json_path):\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            json_dict = json.load(f)\n",
        "        # for label_id in range(len(json_dict[\"shapes\"])):\n",
        "        #     if json_dict[\"shapes\"][label_id][\"label\"] == \"model_pred\": ## no need to add model label again\n",
        "        #         break\n",
        "        #     if label_id == len(json_dict[\"shapes\"]) -1:  ## \"model_pred\" not found in labels\n",
        "        #         json_dict[\"shapes\"].append(pred_coord_label)\n",
        "        # if len(json_dict[\"shapes\"]) == 0:  ## \"model_pred\" not found in labels\n",
        "        json_dict[\"shapes\"].append(pred_coord_label)\n",
        "        json_dict[\"imageHeight\"], json_dict[\"imageWidth\"] = img_size, img_size \n",
        "    else:\n",
        "        json_dict = {\n",
        "            \"version\": \"4.4.1\",\n",
        "            \"flags\": {},\n",
        "            \"shapes\": [pred_coord_label],\n",
        "            \"imagePath\": fnames[0][-9:],\n",
        "            \"imageData\": None,\n",
        "            \"imageHeight\": img_size,\n",
        "            \"imageWidth\": img_size\n",
        "            }\n",
        "    with open(json_path, \"w\") as json_file:\n",
        "        json.dump(json_dict, json_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Postprocess method 1: PCA\n",
        "def mask_to_line(pred_idx, img_size, flag):\n",
        "    with parallel_backend('threading', n_jobs=-2):\n",
        "        pca = PCA(n_components=1)\n",
        "        # Find the line from the points in the binary mask using PCA\n",
        "        # pred_line = mask2Line(pred_idx, pca)\n",
        "        mask_2d = np.stack([pred_idx[0], pred_idx[1]], axis=1)\n",
        "\n",
        "        # Fit PCA on the target mask points\n",
        "        pca.fit(mask_2d)\n",
        "        mask_1d = pca.transform(mask_2d)\n",
        "        mask_2d_new = pca.inverse_transform(mask_1d)\n",
        "        mask_2d_new = sorted(mask_2d_new, key=lambda x: x[0])\n",
        "    \n",
        "    ## clamp to startup x >= 0\n",
        "    for i in range(len(mask_2d_new)):\n",
        "        x_i = mask_2d_new[i][1]\n",
        "        if x_i >= 0:\n",
        "            break\n",
        "    mask_2d_new = mask_2d_new[i:]\n",
        "\n",
        "    mask_line_end_points = [\n",
        "        [ mask_2d_new[0][1],   ## x0\n",
        "        mask_2d_new[0][0],   ## y0\n",
        "        ],  \n",
        "        [ mask_2d_new[-1][1],  ## x1\n",
        "        mask_2d_new[-1][0],  ## y1\n",
        "        ],\n",
        "    ]\n",
        "    mask_line_end_points = np.clip(mask_line_end_points, 0, img_size).tolist()\n",
        "    if mask_line_end_points[0][0] == mask_line_end_points[1][0]:\n",
        "        mask_line_end_points[1][0] += 1\n",
        "    if mask_line_end_points[0][1] == mask_line_end_points[1][1]:\n",
        "        mask_line_end_points[1][1] += 1\n",
        "    pred_coord_label = {\"label\": \"needle\",\n",
        "                        \"points\": mask_line_end_points,\n",
        "                        \"group_id\": None,\n",
        "                        \"shape_type\": \"rectangle\",\n",
        "                        \"flags\":{}}\n",
        "    flag.needle_location = [ mask_2d_new[0][1],  mask_2d_new[0][0],  mask_2d_new[-1][1],  mask_2d_new[-1][0] ]  # [x1,y1,x3,y3]\n",
        "    return pred_coord_label  ## for json\n",
        "\n",
        "## Postprocess method 2: fit a rotate-able minimum area rectangle\n",
        "## default (slightly faster than mask_to_line by PCA)\n",
        "def min_rect_2_line(mask: np.ndarray, flag):\n",
        "    \"\"\"\n",
        "    Fit a minimum area bounding rectangle to a binary mask,\n",
        "    extract the 2 middle points on the shorter edges.\\\\\n",
        "    **flag.needle_location** is set to  **[x1,y1,x3,y3]** \\\\\n",
        "    **NOTE**: this may introduce bias if mask width is unstable. \n",
        "\n",
        "    Args:\n",
        "        mask: Binary mask tensor of shape [H, W], dtype=torch.uint8\n",
        "    Returns:\n",
        "        pred_coord_label: A dictionary to save in json\n",
        "        \t- label: \"needle\"\n",
        "            - points: [[x1,y1],[x3,y3]]\n",
        "\t\t\t- group_id: None\n",
        "\t\t\t- shape_type: \"rectangle\"\n",
        "\t\t\t- flags: {}\n",
        "    \"\"\"\n",
        "    # Convert PyTorch tensor to NumPy\n",
        "    img_size = max(mask.shape[-1], mask.shape[-2])\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if len(contours) == 0:\n",
        "        raise ValueError(\"No contours found in the mask.\")\n",
        "    contours_combined = np.vstack(contours)  ## stack a list of contours\n",
        "    hull = cv2.convexHull(contours_combined)\n",
        "    # Get the minimum area bounding rectangle\n",
        "    # rect = cv2.minAreaRect(contours[0])\n",
        "    rect = cv2.minAreaRect(hull)\n",
        "    # (cx, cy), (width, height), angle = rect  # Center, size (width, height), and rotation angle\n",
        "    # Get the four corner points of the rectangle\n",
        "    box = cv2.boxPoints(rect)\n",
        "    # box = np.int64(np.round(box))  # Convert to integer\n",
        "    \n",
        "    ## pair the coords on short edge together\n",
        "    dist_list, pair = [], [1,2,3]\n",
        "    for c in pair:\n",
        "        dist = pow(box[0][0]-box[c][0],2)+ pow(box[0][1]-box[c][1],2)\n",
        "        dist_list.append(dist)\n",
        "    min_dist_idx = dist_list.index(min(dist_list))+1\n",
        "    pair.remove(min_dist_idx)\n",
        "    pair = [0,min_dist_idx] + pair\n",
        "    \n",
        "    x0,y0 = (box[0][0] + box[pair[1]][0]) / 2, (box[0][1] + box[pair[1]][1]) / 2\n",
        "    x1,y1 = (box[pair[2]][0] + box[pair[3]][0]) / 2, (box[pair[2]][1] + box[pair[3]][1]) / 2\n",
        "    \n",
        "    shorter_edge_midpoints = [x0,y0,x1,y1]\n",
        "    shorter_edge_midpoints = [max(min(x, img_size), 0) for x in shorter_edge_midpoints]  ## clamp\n",
        "    \n",
        "    if shorter_edge_midpoints[0] == shorter_edge_midpoints[2]:\n",
        "        shorter_edge_midpoints[2] += 1\n",
        "    if shorter_edge_midpoints[1] == shorter_edge_midpoints[3]:\n",
        "        shorter_edge_midpoints[3] += 1\n",
        "    mask_line_end_points = [\n",
        "        [ shorter_edge_midpoints[0],   ## x0\n",
        "        shorter_edge_midpoints[1],   ## y0\n",
        "        ],  \n",
        "        [ shorter_edge_midpoints[2],  ## x1\n",
        "        shorter_edge_midpoints[3],  ## y1\n",
        "        ],\n",
        "    ]\n",
        "    pred_coord_label = {\"label\": \"needle\",\n",
        "                        \"points\": mask_line_end_points,\n",
        "                        \"group_id\": None,\n",
        "                        \"shape_type\": \"rectangle\",\n",
        "                        \"flags\":{}}\n",
        "    flag.needle_location = shorter_edge_midpoints  # [x1,y1,x3,y3]\n",
        "    return pred_coord_label  ## for json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Inference helpers (including model prediction & PK)\n",
        "## NOTE: only `inference_LCR_image_with_flag` is updated to the newest version\n",
        "\n",
        "# region ## not used now\n",
        "\n",
        "## input is torch dataloader\n",
        "def inference_loader_with_flag(model, device, loader, flag, save_json=False, save_mask=False):\n",
        "    '''\n",
        "    - Get 3 frames t1/t2/t3 from loader, view them as L/C/R (left/center/right)\n",
        "    - model predicts the needle at C\n",
        "    - PK on L & R frame based on predicted needle to get depth\n",
        "    '''\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # parameters for iteration over the buffer\n",
        "        time_window = 3 #run.config[\"Time Window\"]\n",
        "\n",
        "        # # total steps for evaluation\n",
        "        for buffer in loader:  # loader getitem() returns a buffer\n",
        "            if flag.count1 % flag.fps_reduce == 0:\n",
        "                # Image data\n",
        "                images = buffer[\"images\"][:, :time_window, :, :].to(device)  ## [N=1, T=3, h, w]\n",
        "                fnames = buffer[\"img_path\"][1]   ## [N]\n",
        "                origin_img_size = buffer[\"origin_img_size\"]\n",
        "                img_size = origin_img_size[0].item()\n",
        "                origin_image = buffer[\"origin_images\"][:, :time_window, :, :]\n",
        "                image_array_L = origin_image[:,0,:,:].squeeze()\n",
        "                image_array_C = origin_image[:,1,:,:].squeeze().cpu().numpy()  ## [H,W]\n",
        "                image_array_R = origin_image[:,2,:,:].squeeze()\n",
        "                image_array_LR = torch.concatenate([image_array_L, image_array_R], axis=-1).cpu().numpy()\n",
        "                image_array_LR = cv2.cvtColor(image_array_LR, cv2.COLOR_GRAY2RGB)  ##[H,W*2,3]\n",
        "                \n",
        "                # Forward pass\n",
        "                pred_masks = model(images)  # [N=1, 3, H, W]\n",
        "                if isinstance(pred_masks, dict):  ## Mask2Former-based model\n",
        "                    output_dict = pred_masks\n",
        "                    pred_masks = output_dict[\"pred_masks\"]\n",
        "\n",
        "                if output_dict[\"pred_class\"].item() < 0.5:\n",
        "                    pred_masks.zero_()\n",
        "\n",
        "                # new ver. -----------------------\n",
        "                pred_mask_resize = pred_masks#.unsqueeze(0)\n",
        "                if pred_mask_resize.shape[-1] != img_size:\n",
        "                    pred_mask_resize = v2.functional.resize(pred_mask_resize, (img_size,img_size), interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                # pred_mask_array = (pred_mask_resize > 0.5).detach().to(dtype=torch.uint8)\n",
        "                # pred_mask_array= pred_mask_array.squeeze().cpu().numpy()\n",
        "\n",
        "                pred_mask_tensor = (pred_mask_resize > 0.5).to(dtype=torch.uint8).squeeze()\n",
        "                pred_mask = pred_mask_tensor.detach().cpu().numpy()\n",
        "                if (pred_mask_tensor != 0).sum() < 2:  # -----------------------\n",
        "\n",
        "                ## old ver. # -----------------------\n",
        "                # ## Resize back to image size\n",
        "                # pred_mask_resize = pred_masks[0].unsqueeze(0)\n",
        "                # pred_mask_resize = v2.functional.resize(pred_mask_resize, (img_size,img_size), interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                # pred_mask_array = (pred_mask_resize > 0.5).detach().to(dtype=torch.uint8)\n",
        "                # pred_mask_array= pred_mask_array.squeeze().cpu().numpy()\n",
        "\n",
        "                # pred_mask_tensor = pred_mask_resize.squeeze()\n",
        "                # pred_mask_tensor = (pred_mask_tensor > 0.5).to(dtype=torch.uint8)\n",
        "                # pred_mask = pred_mask_tensor.detach().cpu().numpy()\n",
        "\n",
        "                # ## pred mask to line\n",
        "                # pred_idx = np.where(pred_mask_array > 0.5)  ## coordinates\n",
        "                # if len(pred_idx[0]) <= 1:  # -----------------------\n",
        "                    flag.found_needle = False\n",
        "                    print(f'idx {flag.count1} Needle not found\\n------------')\n",
        "                else:\n",
        "                    flag.found_needle = True\n",
        "                    # pred_coord_label = mask_to_line(pred_idx, img_size, flag)\n",
        "                    pred_coord_label = min_rect_2_line(pred_mask, flag)\n",
        "\n",
        "                    ## save json\n",
        "                    if save_json:\n",
        "                        save_pred_to_json(fnames, pred_coord_label, img_size, save_folder_dir=flag.save_folder_name)\n",
        "                \n",
        "                    # 線段檢查(current_frame_center)，PK(current_frame)\n",
        "                    # current_frame_gray_blur = cv2.blur(image_array_LR, (flag.blur, flag.blur))\n",
        "                    # OutputImg_線段檢查 = 線段檢查(flag, current_frame_gray_blur.copy())\n",
        "                    (OutputImg_PK, \n",
        "                     OutputImg_pk_regression_先左右互扣, \n",
        "                     OutputImg_pk_regression_再左右互扣) = PK(flag, image_array_LR.copy(), return_plot=save_mask, set_train_idx_point=True)\n",
        "\n",
        "                    ## save mask\n",
        "                    if save_mask:\n",
        "                        flag.filename = os.path.basename(fnames[0]).replace(\".jpg\", '_pred.png')\n",
        "                        gt_mask, gt_line = None,None\n",
        "                        save_pred_to_plot(image_array_C, pred_mask_array, flag.needle_location, gt_mask , gt_line, \n",
        "                                            OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣,\n",
        "                                            fname=os.path.join(flag.save_folder_name, flag.filename))\n",
        "                    \n",
        "                    ## Final Estimation\n",
        "                    x1, y1, x3, y3 = flag.needle_location\n",
        "                    # print(f'[cut] {flag.x1_cut_idx} ~ {flag.x3_cut_idx}  [train_end_left] {flag.find_train_end_left}')\n",
        "                    \n",
        "                    if flag.x1_cut_idx < flag.x3_cut_idx:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-1-flag.find_train_end_left]\n",
        "                    else:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-1-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-flag.find_train_end_left]\n",
        "                    print(f'idx {flag.count1}')\n",
        "                    print(f'[x1 y1 raw_z1] {x1:.4f}, {y1:.4f}, {x1y1_regression_depth:.4f}')\n",
        "                    print(f'[x3 y3 raw_z3] {x3:.4f}, {y3:.4f}, {x3y3_regression_depth:.4f}')\n",
        "                    print('------------')\n",
        "\n",
        "            flag.count1 = flag.count1+1\n",
        "            # pbar.update(1)\n",
        "            # if flag.count1 > 100:\n",
        "            #     return\n",
        "\n",
        "## input is Sonosite folder\n",
        "def inference_folder_with_flag(model, device, data_dir, flag, save_json=False, save_mask=False):\n",
        "    '''\n",
        "    - Get 3 frames t1/t2/t3 from loader, view them as L/C/R (left/center/right), and model predicts the needle at C\n",
        "    - PK on L & R frame based on predicted needle\n",
        "    '''\n",
        "    file_names = sorted(os.listdir(data_dir))\n",
        "    image_names = [f for f in file_names if f[0] == \"a\" and f.endswith(\".jpg\")]\n",
        "    lcr_queue = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # parameters for iteration over the buffer\n",
        "        time_window = 3 #run.config[\"Time Window\"]\n",
        "\n",
        "        # # total steps for evaluation\n",
        "        for f in image_names:\n",
        "            image = Image.open(os.path.join(data_dir, f)).convert(\"L\")\n",
        "            ## preprocess\n",
        "            image_tensor = trans_totensor(image)  ## [1,H,W]\n",
        "            img_size = image_tensor.shape[-1]\n",
        "            \n",
        "            lcr_queue.append(image_tensor)\n",
        "            if len(lcr_queue) == 3 and flag.count1 % flag.fps_reduce == 0:\n",
        "                images = torch.stack(lcr_queue, dim=1)\n",
        "                images = v2.functional.resize(images, (config[\"Model\"][\"image_size\"], config[\"Model\"][\"image_size\"]), \n",
        "                                                interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                images = images.to(device)\n",
        "                fnames = [f]\n",
        "\n",
        "                image_array_L = lcr_queue[0].squeeze()\n",
        "                image_array_C = lcr_queue[1].squeeze()  ## [H,W]\n",
        "                image_array_R = lcr_queue[2].squeeze()\n",
        "                image_array_LR = torch.concatenate([image_array_L, image_array_R], axis=-1).cpu().numpy()\n",
        "                image_array_LR = cv2.cvtColor(image_array_LR, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "                # Forward pass\n",
        "                pred_masks = model(images)  # [N=1, 3, h, w]\n",
        "                if isinstance(pred_masks, dict):  ## Mask2Former-based model\n",
        "                    output_dict = pred_masks\n",
        "                    pred_masks = output_dict[\"pred_masks\"]\n",
        "                    if output_dict[\"pred_class\"].item() < 0.5:\n",
        "                        pred_masks.zero_()\n",
        "\n",
        "                ## new ver. remove pred_idx (rely on .sum) ## -------------\n",
        "                # ## Resize back to image size  \n",
        "                pred_mask_resize = pred_masks#.unsqueeze(0)\n",
        "                if pred_mask_resize.shape[-1] != img_size:\n",
        "                    pred_mask_resize = v2.functional.resize(pred_mask_resize, (img_size,img_size), interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                # pred_mask_array = (pred_mask_resize > 0.5).detach().to(dtype=torch.uint8)\n",
        "                # pred_mask_array= pred_mask_array.squeeze().cpu().numpy()\n",
        "\n",
        "                pred_mask_tensor = (pred_mask_resize > 0.5).to(dtype=torch.uint8).squeeze()\n",
        "                pred_mask = pred_mask_tensor.detach().cpu().numpy()\n",
        "                if (pred_mask_tensor != 0).sum() < 2:\n",
        "                    flag.found_needle = False\n",
        "                    print(f'idx {flag.count1} Needle not found\\n------------')\n",
        "                else:\n",
        "                    flag.found_needle = True\n",
        "                    # pred_coord_label = mask_to_line(pred_idx, img_size, flag)\n",
        "                    pred_coord_label = min_rect_2_line(pred_mask, flag) ## this is faster\n",
        "\n",
        "                    ## save json\n",
        "                    if save_json:\n",
        "                        save_pred_to_json(fnames, pred_coord_label, img_size, save_folder_dir=flag.save_folder_name)\n",
        "                \n",
        "                    # 線段檢查(current_frame_center)，PK(current_frame)\n",
        "                    # current_frame_gray_blur = cv2.blur(image_array_LR, (flag.blur, flag.blur))\n",
        "                    # OutputImg_線段檢查 = 線段檢查(flag, current_frame_gray_blur.copy())\n",
        "                    (OutputImg_PK, \n",
        "                     OutputImg_pk_regression_先左右互扣, \n",
        "                     OutputImg_pk_regression_再左右互扣) = PK(flag, image_array_LR.copy(), return_plot=save_mask, set_train_idx_point=True)\n",
        "\n",
        "                    ## save mask\n",
        "                    if save_mask:\n",
        "                        flag.filename = os.path.basename(fnames[0]).replace(\".jpg\", '_pred.png')\n",
        "                        gt_mask, gt_line = None,None\n",
        "                        save_pred_to_plot(image_array_C, pred_mask, flag.needle_location, gt_mask , gt_line, \n",
        "                                            OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣,\n",
        "                                            fname=os.path.join(flag.save_folder_name, flag.filename))\n",
        "                    \n",
        "                    ## Final Estimation\n",
        "                    x1, y1, x3, y3 = flag.needle_location\n",
        "                    # print(f'[cut] {flag.x1_cut_idx} ~ {flag.x3_cut_idx}  [train_end_left] {flag.find_train_end_left}')\n",
        "                    \n",
        "                    if flag.x1_cut_idx < flag.x3_cut_idx:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-1-flag.find_train_end_left]\n",
        "                    else:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-1-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-flag.find_train_end_left]\n",
        "                    print(f'idx {flag.count1}')\n",
        "                    print(f'[x1 y1 raw_z1] {x1:.4f}, {y1:.4f}, {x1y1_regression_depth:.4f}')\n",
        "                    print(f'[x3 y3 raw_z3] {x3:.4f}, {y3:.4f}, {x3y3_regression_depth:.4f}')\n",
        "                    print('------------')\n",
        "\n",
        "            if len(lcr_queue) == 3:\n",
        "                flag.count1 = flag.count1+1\n",
        "                lcr_queue.pop(0)  ## remove the first frame\n",
        "            # pbar.update(1)\n",
        "            # if flag.count1 > 100:\n",
        "            #     return\n",
        "\n",
        "## input is processed Prodigy video (crop into 3 videos)\n",
        "def inference_LCRfolder_with_flag(model, device, data_dir, flag, save_json=False, save_mask=False):\n",
        "    '''\n",
        "    - Get same file name image from folder L/C/R (left/center/right), and model predicts the needle at C\n",
        "    - PK on L & R frame based on predicted needle\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        lcr_dir_path = [data_dir+\"/L\", data_dir+\"/C\", data_dir+\"/R\"]\n",
        "\n",
        "        file_names = sorted(os.listdir(data_dir+\"/L\"))\n",
        "        image_names = [f for f in file_names if f[0] == \"a\" and f.endswith(\".jpg\")]  # [\"a0001.jpg\", \"a0002.jpg\", ...]\n",
        "        \n",
        "        for f in image_names:\n",
        "            fname_list, lcr_queue = [],[]\n",
        "            for dir_path in lcr_dir_path:\n",
        "                img = Image.open(os.path.join(dir_path, f)).convert(\"L\")\n",
        "                img_tensor = trans_totensor(img)\n",
        "                lcr_queue.append(img_tensor)\n",
        "                fname_list.append(os.path.join(dir_path, f))\n",
        "                img.close()\n",
        "            # print(fname_list)\n",
        "            img_size = img_tensor.shape[-1]\n",
        "            \n",
        "            if flag.count1 % flag.fps_reduce == 0:\n",
        "                images = torch.stack(lcr_queue, dim=1)\n",
        "                images = v2.functional.resize(images, (config[\"Model\"][\"image_size\"], config[\"Model\"][\"image_size\"]), \n",
        "                                                interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                images = images.to(device)\n",
        "                fnames = [f]\n",
        "\n",
        "                ## get numpy array for PK\n",
        "                image_array_L = lcr_queue[0].squeeze()\n",
        "                image_array_C = lcr_queue[1].squeeze()  ## [H,W]\n",
        "                image_array_R = lcr_queue[2].squeeze()\n",
        "                image_array_LR = torch.concatenate([image_array_L, image_array_R], axis=-1).cpu().numpy()\n",
        "                image_array_LR = cv2.cvtColor(image_array_LR, cv2.COLOR_GRAY2RGB)  ## 0.~1.\n",
        "                # print('[image_array_LR]', torch.max(image_array_LR),torch.min(image_array_LR), len(torch.unique(image_array_LR)))\n",
        "\n",
        "                # Forward pass\n",
        "                pred_masks = model(images)  # [N=1, 3, h, w]\n",
        "                if isinstance(pred_masks, dict):  ## Mask2Former-based model\n",
        "                    output_dict = pred_masks\n",
        "                    pred_masks = output_dict[\"pred_masks\"]\n",
        "                    if output_dict[\"pred_class\"].item() < 0.5:\n",
        "                        pred_masks.zero_()\n",
        "\n",
        "                ## new ver. remove pred_idx (rely on .sum) ## -------------\n",
        "                # ## Resize back to image size  \n",
        "                pred_mask_resize = pred_masks#.unsqueeze(0)\n",
        "                if pred_mask_resize.shape[-1] != img_size:\n",
        "                    pred_mask_resize = v2.functional.resize(pred_mask_resize, (img_size,img_size), interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "                # pred_mask_array = (pred_mask_resize > 0.5).detach().to(dtype=torch.uint8)\n",
        "                # pred_mask_array= pred_mask_array.squeeze().cpu().numpy()\n",
        "\n",
        "                pred_mask_tensor = (pred_mask_resize > 0.5).to(dtype=torch.uint8).squeeze()\n",
        "                pred_mask = pred_mask_tensor.detach().cpu().numpy()\n",
        "                \n",
        "                if (pred_mask_tensor != 0).sum() < 2: #len(pred_idx[0]) <= 1:  ## -------------\n",
        "                    flag.found_needle = False\n",
        "                    print(f'idx {flag.count1} Needle not found\\n------------')\n",
        "                else:\n",
        "                    flag.found_needle = True\n",
        "                    # pred_coord_label = mask_to_line(pred_idx, img_size, flag)\n",
        "                    pred_coord_label = min_rect_2_line(pred_mask, flag)\n",
        "\n",
        "                    ## save json\n",
        "                    if save_json:\n",
        "                        save_pred_to_json(fnames, pred_coord_label, img_size, save_folder_dir=flag.save_folder_name)\n",
        "                \n",
        "                    # 線段檢查(current_frame_center)，PK(current_frame)\n",
        "                    # current_frame_gray_blur = cv2.blur(image_array_LR, (flag.blur, flag.blur))\n",
        "                    # OutputImg_線段檢查 = 線段檢查(flag, current_frame_gray_blur.copy())\n",
        "                    (OutputImg_PK, \n",
        "                     OutputImg_pk_regression_先左右互扣, \n",
        "                     OutputImg_pk_regression_再左右互扣) = PK(flag, image_array_LR.copy(), return_plot=save_mask, set_train_idx_point=True)\n",
        "\n",
        "                    ## save mask\n",
        "                    if save_mask:\n",
        "                        flag.filename = os.path.basename(fnames[0]).replace(\".jpg\", '_pred.png')\n",
        "                        gt_mask, gt_line = None,None\n",
        "                        save_pred_to_plot(image_array_C, pred_mask, flag.needle_location, gt_mask , gt_line, \n",
        "                                            OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣,\n",
        "                                            fname=os.path.join(flag.save_folder_name, flag.filename))\n",
        "                    \n",
        "                    ## Final Estimation\n",
        "                    x1, y1, x3, y3 = flag.needle_location\n",
        "                    # print(f'[cut] {flag.x1_cut_idx} ~ {flag.x3_cut_idx}  [train_end_left] {flag.find_train_end_left}')\n",
        "                    \n",
        "                    if flag.x1_cut_idx < flag.x3_cut_idx:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-1-flag.find_train_end_left]\n",
        "                    else:\n",
        "                        x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-1-flag.find_train_end_left]\n",
        "                        x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-flag.find_train_end_left]\n",
        "                    print(f'idx {flag.count1}')\n",
        "                    print(f'[x1 y1 raw_z1] {x1:.4f}, {y1:.4f}, {x1y1_regression_depth:.4f}')\n",
        "                    print(f'[x3 y3 raw_z3] {x3:.4f}, {y3:.4f}, {x3y3_regression_depth:.4f}')\n",
        "                    print('------------')\n",
        "\n",
        "                    # plt.figure(figsize=(4, 4))\n",
        "                    # plt.imshow((image_array_C*255), cmap='gray', vmin=0, vmax=255) # vmin/vmax for 8-bit grayscale\n",
        "                    # plt.plot([x1,x3], [y1,y3], 'ro', markersize=8) # 'ro' for red circles, markersize for dot size\n",
        "                    # plt.tight_layout()\n",
        "                    # plt.margins(x=0, y=0)\n",
        "                    # plt.axis('off') # Turn off axis ticks and labels if not needed for the image\n",
        "                    # plt.show()\n",
        "\n",
        "            if len(lcr_queue) == 3:\n",
        "                flag.count1 = flag.count1+1\n",
        "                lcr_queue.pop(0)  ## remove the first frame\n",
        "            # pbar.update(1)\n",
        "            # if flag.count1 > 100:\n",
        "            #     return\n",
        "\n",
        "# endregion\n",
        "\n",
        "## input is online frame cropped from raw Prodigy video\n",
        "def inference_LCR_image_with_flag(model, device, lcr_image, flag, save_json=False, save_mask=False):\n",
        "    '''\n",
        "    - Input list of images L/C/R (left/center/right)\n",
        "    - Model predicts the needle at C\n",
        "    - PK on L & R frame based on predicted needle to get depth\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        if flag.count1 % flag.fps_reduce == 0:\n",
        "            lcr_queue = []\n",
        "            image_array_L = lcr_image[0][:, :, ::-1].astype(np.float32) / 255.0\n",
        "            image_array_R = lcr_image[2][:, :, ::-1].astype(np.float32) / 255.0  ## BGR to RGB\n",
        "            \n",
        "\t\t\t## Stack LCR to channel=3 image ----------------------------------------------------------\n",
        "            for idx in range(len(lcr_image)):\n",
        "                lcr_image[idx] = cv2.cvtColor(lcr_image[idx], cv2.COLOR_BGR2GRAY )  ## [H,W,3]->[H,W]\n",
        "                lcr_image[idx] = lcr_image[idx].astype(np.float32) / 255.0\n",
        "                ## GPT said CV2 resize is faster than torch resize\n",
        "                img_tensor = cv2.resize(lcr_image[idx], (config[\"Model\"][\"image_size\"], config[\"Model\"][\"image_size\"]), interpolation=cv2.INTER_LINEAR)\n",
        "                img_tensor = torch.from_numpy(img_tensor).unsqueeze(0) # [1,H,W]\n",
        "                lcr_queue.append(img_tensor)\n",
        "\n",
        "            images = torch.stack(lcr_queue, dim=1)  ## resized to training size (384) [1,3,h,w]\n",
        "            images = images.to(device)\n",
        "            img_size = lcr_image[idx].shape[-1]     ## origin input size (512)\n",
        "\n",
        "            ## Get numpy array for PK -----------------------------------------------------------------\n",
        "            image_array_C = lcr_image[1].squeeze()  ## [H,W]\n",
        "            image_array_LR = np.concatenate([image_array_L, image_array_R], axis=1)\n",
        "\n",
        "            ## Model prediction -----------------------------------------------------------------------\n",
        "            pred_masks = model(images)  # [N=1, 3, h, w]\n",
        "            if isinstance(pred_masks, dict):\n",
        "                output_dict = pred_masks\n",
        "                pred_masks = output_dict[\"pred_masks\"]\n",
        "                if output_dict[\"pred_class\"].item() < 0.5:\n",
        "                    pred_masks.zero_()\n",
        "\n",
        "            ## Resize back to input size  -------------------------------------------------------------\n",
        "            pred_mask_resize = pred_masks#.unsqueeze(0)\n",
        "            if pred_mask_resize.shape[-1] != img_size:\n",
        "                pred_mask_resize = v2.functional.resize(pred_mask_resize, (img_size,img_size), interpolation=tf.InterpolationMode.BILINEAR, antialias=True)\n",
        "\n",
        "            pred_mask_tensor = (pred_mask_resize > 0.5).to(dtype=torch.uint8).squeeze()\n",
        "            pred_mask = pred_mask_tensor.detach().cpu().numpy()\n",
        "            if flag.count1 == 0:\n",
        "                print('[pred_mask]', pred_mask.shape)\n",
        "            \n",
        "            if (pred_mask_tensor != 0).sum() < 2: ## Predict empty mask -------------------------------\n",
        "                flag.found_needle = False\n",
        "                print(f'idx {flag.count1} Needle not found\\n------------')\n",
        "            else:\n",
        "                ## Post process mask into endpoints ---------------------------------------------------\n",
        "                flag.found_needle = True\n",
        "                # pred_coord_label = mask_to_line(pred_idx, img_size, flag)\n",
        "                pred_coord_label = min_rect_2_line(pred_mask, flag) ## pred_coord_label is a dict to save as json if save_json\n",
        "\n",
        "                ## Endpoint Prediction =====================\n",
        "                x1, y1, x3, y3 = flag.needle_location     \n",
        "                ## =========================================\n",
        "                \n",
        "\t\t\t\t## Plot prediciton --------------------------------------------------------------------\n",
        "                # print(f'[cut] {flag.x1_cut_idx} ~ {flag.x3_cut_idx}  [train_end_left] {flag.find_train_end_left}')\n",
        "\t\t\t\t# plt.figure(figsize=(4, 4))\n",
        "                # plt.imshow((image_array_C*255), cmap='gray', vmin=0, vmax=255) # vmin/vmax for 8-bit grayscale\n",
        "                # plt.plot([x1,x3], [y1,y3], 'ro', markersize=8) # 'ro' for red circles, markersize for dot size\n",
        "                # plt.tight_layout()\n",
        "                # plt.margins(x=0, y=0)\n",
        "                # plt.axis('off') # Turn off axis ticks and labels if not needed for the image\n",
        "                # plt.show()\n",
        "\n",
        "                ## PK to estimate depth (z1 z3) -------------------------- ###########################\n",
        "                (OutputImg_PK, \n",
        "                    OutputImg_pk_regression_先左右互扣, \n",
        "                    OutputImg_pk_regression_再左右互扣) = PK(flag, image_array_LR.copy(), return_plot=save_mask, set_train_idx_point=True)\n",
        "\t\t\t\t\n",
        "\t\t\t\t## Depth Estimation ======================================\n",
        "                if flag.x1_cut_idx < flag.x3_cut_idx:\n",
        "                    x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-flag.find_train_end_left]\n",
        "                    x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-1-flag.find_train_end_left]\n",
        "                else:\n",
        "                    x1y1_regression_depth = flag.pk_regression_box_再左右互扣[flag.x1_cut_idx-1-flag.find_train_end_left]\n",
        "                    x3y3_regression_depth = flag.pk_regression_box_再左右互扣[flag.x3_cut_idx-flag.find_train_end_left]\n",
        "                ## =======================================================\n",
        "                \n",
        "                print(f'idx {flag.count1}')\n",
        "                print(f'[x1 y1 raw_z1] {x1:.4f}, {y1:.4f}, {x1y1_regression_depth:.4f}')\n",
        "                print(f'[x3 y3 raw_z3] {x3:.4f}, {y3:.4f}, {x3y3_regression_depth:.4f}')\n",
        "                print('------------')\n",
        "\n",
        "                ## Save mask -------------------------------------------------------------------------------\n",
        "                frame_id = f\"{flag.count1:05d}\"\n",
        "                if save_mask:\n",
        "                    predmask_filename = \"pred_\" + frame_id + '.png'\n",
        "                    gt_mask, gt_line = None,None\n",
        "                    save_pred_to_plot(image_array_C, pred_mask, flag.needle_location, gt_mask , gt_line, \n",
        "                                        OutputImg_PK, OutputImg_pk_regression_先左右互扣, OutputImg_pk_regression_再左右互扣,\n",
        "                                        fname=os.path.join(flag.save_folder_name, predmask_filename))\n",
        "                ## Save json -------------------------------------------------------------------------------\n",
        "                if save_json:\n",
        "                    save_pred_to_json([\"pred_\" + frame_id + '.json'], pred_coord_label, img_size, save_folder_dir=flag.save_folder_name)\n",
        "                    \n",
        "        flag.count1 = flag.count1 + 1\n",
        "    return flag   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output example:\n",
        "```\n",
        "idx <t>\n",
        "[x1 y1 raw_z1] (7.598571007578613, 250.50608134443533, -0.23366086184978663)\n",
        "[x3 y3 raw_z3] (184.2970108538728, 331.3508378681138, -11.399152440950273)\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "In `inference_loader_with_flag()`, the model predicts x1y1 & x3y3 for frame t, and PK regresssion estimates the depth (denoted as `raw_z1` & `raw_z3`).\n",
        "\n",
        "`raw_z1` & `raw_z3` can be scaled manually in 3D construction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55a55FjP1onU"
      },
      "outputs": [],
      "source": [
        "## track model\n",
        "if __name__ == \"__main__\":\n",
        "\tcount = 0\n",
        "\t## Set the image raw size\n",
        "\tif not (width > 0 and height > 0):\n",
        "\t\tif '.mp4' not in data_dir:\n",
        "\t\t\tfor f in os.listdir(data_dir):\n",
        "\t\t\t\tif f.lower().endswith(\".jpg\"):\n",
        "\t\t\t\t\timage = Image.open(os.path.join(data_dir, f))\n",
        "\t\t\t\t\twidth, height = image.size\n",
        "\t\t\t\t\tbreak\n",
        "\tflag = Flag(cutNum=cutNum, height=height, width=width)\n",
        "\t\n",
        "\t## User Settings\n",
        "\tflag.run_only_one = run_only_one\n",
        "\tif save_json or save_mask:\n",
        "\t\tdata_dir = data_dir.replace('.mp4','')\n",
        "\t\tflag.save_folder_name = data_dir + '_PK'\n",
        "\t\tif not os.path.exists(flag.save_folder_name):\n",
        "\t\t\tos.makedirs(flag.save_folder_name)\n",
        "\t\tprint(f'[Path] Results are saved to {flag.save_folder_name}')\n",
        "\telse:\n",
        "\t\tprint('...No extra files are saved')\n",
        "\n",
        "\t## Reset model ----------------------------------\n",
        "\tframe_count = 0\n",
        "\tmodel.eval()\n",
        "\tmodel.compile() ### torch.jit.script will cause error so just use compile  \n",
        "\tmodel.to(device)\n",
        "\tflag.count1 = 0\n",
        "\n",
        "\t### Reset Memory for track model -----------------\n",
        "\tif isinstance(model, MemInferenceWrapper):\n",
        "\t\tmodel.clear_memory()\n",
        "\t\tprint(f\"[Model reset memory] memory_engaged: {model.memory_engaged()}\")\n",
        "\n",
        "\t#主迴圈開始\n",
        "\t## Inference on raw prodigy video -------------------------------\n",
        "\tif preprocess_video:\n",
        "\t\t## Default for Prodigy video: Single video path, need to crop each frame into 3 frames ======================\n",
        "\t\tif isinstance(config_PK.Data.raw_video_dir, str):\n",
        "\t\t\t# flag.filename = config_PK.Data.raw_video_dir\n",
        "\t\t\tcap = cv2.VideoCapture(config_PK.Data.raw_video_dir)\n",
        "\t\t\tassert cap.isOpened(), \"Error opening video file\"\n",
        "\n",
        "\t\t\ttarget_fps = config_PK.User_setting.target_fps\n",
        "\t\t\tprint(f\"[VIDEO] Cut at fps {target_fps}\")\n",
        "\t\t\toriginal_fps = cap.get(cv2.CAP_PROP_FPS)  ## Prodigy video:40\n",
        "\t\t\tframe_interval = int(round(original_fps / target_fps))\n",
        "\t\t\tprint(f\"[VIDEO] original fps {original_fps}\")\n",
        "\n",
        "\n",
        "\t\t\t## capture frames --------------------------------------\n",
        "\t\t\twhile True:\n",
        "\t\t\t\tret, frame = cap.read()\n",
        "\t\t\t\tif not ret:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\t\n",
        "\t\t\t\tif frame_count % frame_interval == 0:\n",
        "\t\t\t\t\t## crop to 3 frames --------------------------------------\n",
        "\t\t\t\t\t# print('[frame]', frame.shape)\n",
        "\t\t\t\t\tframe_l = frame[y_1:y_2, x_l1:x_l2]\n",
        "\t\t\t\t\tframe_c = frame[y_1:y_2, x_c1:x_c2]\n",
        "\t\t\t\t\tframe_r = frame[y_1:y_2, x_r1:x_r2]\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\t## Threading --------------------------------------------\n",
        "\t\t\t\t\t## https://stackoverflow.com/a/58829816\n",
        "\t\t\t\t\twith concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "\t\t\t\t\t\tinference_thread = executor.submit(inference_LCR_image_with_flag, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tmodel, device, [frame_l, frame_c, frame_r], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tflag, save_json, save_mask)\n",
        "\t\t\t\t\t\tflag = inference_thread.result()\n",
        "\t\t\t\t\t# flag = inference_LCR_image_with_flag(model, device, [frame_l, frame_c, frame_r], \n",
        "\t\t\t\t\t#                                     flag, save_json, save_mask)\n",
        "\t\t\t\tframe_count += 1\n",
        "\n",
        "\t\t\tcap.release()\n",
        "\t\t\tcv2.destroyAllWindows()\n",
        "\t\t## ==========================================================================================================\n",
        "\n",
        "\t\t## 3 video paths (left, center, right)\n",
        "\t\telif (isinstance(config_PK.Data.raw_video_dir, list) or \n",
        "\t\t\tisinstance(config_PK.Data.raw_video_dir, omegaconf.listconfig.ListConfig)):  \n",
        "\t\t\tcap_l = cv2.VideoCapture(config_PK.Data.raw_video_dir[0])\n",
        "\t\t\tcap_c = cv2.VideoCapture(config_PK.Data.raw_video_dir[1])\n",
        "\t\t\tcap_r = cv2.VideoCapture(config_PK.Data.raw_video_dir[2])\n",
        "\t\t\tassert cap_l.isOpened(), \"Error opening video file\"\n",
        "\n",
        "\t\t\ttarget_fps = 15\n",
        "\t\t\tsource_fps = cap_l.get(cv2.CAP_PROP_FPS)\n",
        "\t\t\tprint(f\"[VIDEO] Cut at fps {target_fps}\")\n",
        "\t\t\toriginal_fps = cap_l.get(cv2.CAP_PROP_FPS)\n",
        "\t\t\tframe_interval = int(round(original_fps / target_fps))        \n",
        "\n",
        "\t\t\twhile True:\n",
        "\t\t\t\tret_l, frame_l = cap_l.read()\n",
        "\t\t\t\tret_c, frame_c = cap_c.read()\n",
        "\t\t\t\tret_r, frame_r = cap_r.read()\n",
        "\t\t\t\tif not ret_l:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\t\n",
        "\t\t\t\tif frame_count % frame_interval == 0:\n",
        "\t\t\t\t\t## Threading: https://stackoverflow.com/a/58829816\n",
        "\t\t\t\t\twith concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "\t\t\t\t\t\tinference_thread = executor.submit(inference_LCR_image_with_flag, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tmodel, device, [frame_l, frame_c, frame_r], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tflag, save_json, save_mask)\n",
        "\t\t\t\t\t\tflag = inference_thread.result()\n",
        "\t\t\t\t\t# flag = inference_LCR_image_with_flag(model, device, [frame_l, frame_c, frame_r], \n",
        "\t\t\t\t\t#                                     flag, save_json, save_mask)\n",
        "\t\t\t\tframe_count += 1\n",
        "\n",
        "\t\t\tcap_l.release()\n",
        "\t\t\tcap_c.release()\n",
        "\t\t\tcap_r.release()\n",
        "\t\t\tcv2.destroyAllWindows()\n",
        "\t\n",
        "\t## Inference on a folder of frames ---------------------\n",
        "\telif not flag.run_only_one:\n",
        "\t\tvideo_dataset = UnlabeledDataset(data_dir, transform=valid_transform,\n",
        "\t\t\t\t\t\t\t\t\t\ttime_window=3,\n",
        "\t\t\t\t\t\t\t\t\t\tbuffer_num_sample=1)\n",
        "\t\t\n",
        "\t\t## Test batch size 1\n",
        "\t\tloader = DataLoader(video_dataset, batch_size=1, shuffle=False, drop_last=False, \n",
        "\t\t\t\t\t\t\tnum_workers=4, persistent_workers=True, pin_memory=True)\n",
        "\t\tprint(f\"[Data] video length:{len(video_dataset)}\")\n",
        "\n",
        "\t\t## Similar to evaluate function but writes points to json file or plots PK result\n",
        "\t\t# inference_loader_with_flag(model, device, loader, flag, save_json, save_mask)\n",
        "\t\t# inference_folder_with_flag(model, device, data_dir, flag, save_json, save_mask)\n",
        "\t\tinference_LCRfolder_with_flag(model, device, data_dir, flag, save_json, save_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Utilization Memo\n",
        "- Sonosite 638 frames, convnext m2f\n",
        "    - pure inference +parallel_backend in mask2line: 74.9s\n",
        "    - inf+PK w/o plot:\n",
        "        * power(最佳效能): 89.7s\n",
        "        * +parallel_backend in regression: 83.8s (critical)\n",
        "        * min_rect_2_line & remove duplicate .cpu(): 65.7s (critical)\n",
        "        * ''  + dynamic tanh: 69.7s (no help)\n",
        "        * model.compile(): 60.3s (critical)\n",
        "            * mode=\"reduce-overhead\":96.0s\n",
        "            * mode=\"max-autotune\":fail\n",
        "\n",
        "        * battery(平衡) +parallel_backend in regression & mask2line: 103.7s\n",
        "\n",
        "        * place data under local demo folder: not helpful\n",
        "        * Direct Image.open without Dataset: 109.9s\n",
        "\n",
        "    - inf w/o PK & plot: 51.8s\n",
        "    - 10 main function avg time:\n",
        "        * min_rect_2_line & remove duplicate .cpu(): 59.75s\n",
        "        * ''  + dynamic tanh: 59.83s\n",
        "        * model.compile(): 51.1s\n",
        "\n",
        "- Prodigy video (1327 frames)\n",
        "    - online capture 3 videos: 175.6s\n",
        "    - online capture 3 videos, compile(): 163.7s (critical)\n",
        "    - online capture 3 videos, compile(), no resize: 412.0s (poor)\n",
        "    - online capture 3 videos, compile(), cv2 resize instead of torch: 167.2s\n",
        "    - online capture 3 videos, compile(), remove duplicate gray2rgb: 122.3s (critical!!!)\n",
        "        - TIP! directly use bgr to rgb in inference function, rather than use bgr to gray to rgb\n",
        "        - now only applied in inference_LCR_image_with_flag()\n",
        "    - online capture 3 videos, compile(), remove duplicate gray2rgb,ThreadPoolExecutor: 105.5s(critical!!!)\n",
        "        - TODO: need to check memory sequence if using mem_m2f (tracking model)\n",
        "    \n",
        "\t- memory m2f (1180 frames due to 40 fps):118.s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvcc -V\n",
        "# !nvidia-smi\n",
        "import torch, os\n",
        "torch.__version__\n",
        "print(\"is_available\", torch.cuda.is_available())\n",
        "# print(torch.cuda.get_arch_list())\n",
        "# print(os.environ.get('TORCH_CUDA_ARCH_LIST'))\n",
        "# print(torch.__path__ )\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Source Code Archive\n",
        "no need to run below code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### video to frames\n",
        "cap = cv2.VideoCapture(config_PK.Data.raw_video_dir)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#for j in range(0,2):    #range是做的總frame跨距(30fps)，fps_reduce每n個做一次\n",
        "for j in range(0,100):   # range設很大會自動停止\n",
        "    flag.count1 = 100*j  # 100 frame存一個資料夾，共200 frame\n",
        "    print(\"j=\",j)\n",
        "    for i in range(100*j*flag.fps_reduce, 100*j*flag.fps_reduce+100*flag.fps_reduce):    #range是做的總frame跨距(30fps)，fps_reduce每n個做一次\n",
        "        ret, current_frame = cap.read()\n",
        "        if ret:\n",
        "            count=count+1\n",
        "            flag.height, flag.width, __  = current_frame.shape\n",
        "            #if count % flag.fps_reduce == 1:               #要跳過幾個frame數在參數區  取餘數\n",
        "            if True:                             #flag.fps_reduce = 1全做\n",
        "                print(\"count1=\",flag.count1)\n",
        "                flag.filename='img{:04}'.format(flag.count1)\n",
        "                if True:\n",
        "                #if 115 == flag.count1 or flag.count1 == 75:\n",
        "                    flag, OutputImg_vconcat = get_sinogram_img(flag, current_frame)\n",
        "                    cv2.imwrite('/content/dataset_auto/'+ flag.filename + '_vconcat.jpg', OutputImg_vconcat, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "                    ##每次存檔都在資料夾名稱加數字(3D每100迴圈要幾小時，就要存檔一次)\n",
        "                flag.count1=flag.count1+1\n",
        "        else:\n",
        "            break\n",
        "    flag.save_func(j) #要打開，這樣每個for迴圈都會存檔   改參數想重跑，不想覆蓋舊的就要關掉?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "needle_seg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
